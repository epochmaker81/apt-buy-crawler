import os
import requests
import pandas as pd
import gspread
from gspread_dataframe import set_with_dataframe
import xml.etree.ElementTree as ET
import time
import json
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import logging
import traceback

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ì„¤ì • ë³€ìˆ˜ ---
SERVICE_KEY = os.getenv('SERVICE_KEY')
GOOGLE_CREDENTIALS_JSON = os.getenv('GOOGLE_CREDENTIALS_JSON')
GOOGLE_SHEET_NAME = 'ì „êµ­ ì•„íŒŒíŠ¸ ë§¤ë§¤ ì‹¤ê±°ë˜ê°€_ëˆ„ì '
LAWD_CODE_FILE = 'lawd_code.csv'
BASE_URL = 'https://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade'

# ì‹¤í–‰ ëª¨ë“œ ì„¤ì •
RUN_MODE = os.getenv('RUN_MODE', 'TEST')

# ë‚ ì§œ ì„¤ì •
today_kst = datetime.utcnow() + timedelta(hours=9)

if RUN_MODE == 'TEST':
    MONTHS_TO_FETCH = [today_kst.strftime('%Y%m')]
    TARGET_REGIONS = ['11110', '11140', '11170']  # ì„œìš¸ 3ê°œ êµ¬
elif RUN_MODE == 'QUICK':
    MONTHS_TO_FETCH = [today_kst.strftime('%Y%m')]
    TARGET_REGIONS = None
else:
    MONTHS_TO_FETCH = []
    for i in range(3):
        target_date = today_kst - relativedelta(months=i)
        MONTHS_TO_FETCH.append(target_date.strftime('%Y%m'))
    TARGET_REGIONS = None

def get_google_creds():
    """Google ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    if GOOGLE_CREDENTIALS_JSON is None:
        logger.error("âŒ GOOGLE_CREDENTIALS_JSONì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    try:
        creds = json.loads(GOOGLE_CREDENTIALS_JSON)
        logger.info(f"âœ… Google ì¸ì¦ ì •ë³´ ë¡œë“œ ì„±ê³µ - í”„ë¡œì íŠ¸: {creds.get('project_id', 'Unknown')}")
        return creds
    except json.JSONDecodeError as e:
        logger.error(f"âŒ GOOGLE_CREDENTIALS_JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

def get_lawd_codes(filepath):
    """ì§€ì—­ ì½”ë“œ íŒŒì¼ ì½ê¸°"""
    try:
        if not os.path.exists(filepath):
            logger.error(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filepath}")
            return []
        
        df = pd.read_csv(filepath)
        logger.info(f"ğŸ“ CSV íŒŒì¼ ì½ê¸° ì„±ê³µ: {len(df)}ê°œ ì§€ì—­")
        
        if 'code' not in df.columns:
            logger.error("âŒ 'code' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        codes = df['code'].astype(str).str.strip().tolist()
        
        # ìœ íš¨í•œ ì½”ë“œë§Œ í•„í„°ë§
        valid_codes = []
        for code in codes:
            if code and code != 'nan' and len(code) == 5 and code.isdigit():
                if not code.endswith('000'):  # ê´‘ì—­ì‹œë„ ì½”ë“œ ì œì™¸
                    valid_codes.append(code)
        
        logger.info(f"âœ… ìœ íš¨í•œ ì§€ì—­ ì½”ë“œ: {len(valid_codes)}ê°œ")
        
        # í•„í„°ë§ ì ìš©
        if TARGET_REGIONS:
            codes = [c for c in valid_codes if c in TARGET_REGIONS]
            logger.info(f"ğŸ¯ TARGET_REGIONS í•„í„°ë§ í›„: {len(codes)}ê°œ - {codes}")
        elif RUN_MODE == 'QUICK':
            major_city_prefixes = ['11', '26', '27', '28', '29', '30', '31', '36']
            codes = [c for c in valid_codes if any(c.startswith(p) for p in major_city_prefixes)]
            logger.info(f"ğŸ™ï¸ ì£¼ìš” ë„ì‹œ í•„í„°ë§ í›„: {len(codes)}ê°œ")
        else:
            codes = valid_codes
        
        if not codes:
            logger.error("âŒ ì²˜ë¦¬í•  ìœ íš¨í•œ ì§€ì—­ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        return codes
        
    except Exception as e:
        logger.error(f"âŒ ì§€ì—­ ì½”ë“œ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return []

def test_api_connection(lawd_cd, deal_ymd, service_key):
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    logger.info(f"ğŸ§ª API ì—°ê²° í…ŒìŠ¤íŠ¸: {lawd_cd} - {deal_ymd}")
    
    params = {
        'serviceKey': service_key,
        'LAWD_CD': lawd_cd,
        'DEAL_YMD': deal_ymd,
        'pageNo': '1',
        'numOfRows': '5'
    }
    
    try:
        response = requests.get(BASE_URL, params=params, timeout=30)
        logger.info(f"ğŸ“¡ HTTP ìƒíƒœ: {response.status_code}")
        
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            result_code = root.find('.//resultCode')
            result_msg = root.find('.//resultMsg')
            
            if result_code is not None:
                code = result_code.text
                msg = result_msg.text if result_msg is not None else "ë©”ì‹œì§€ ì—†ìŒ"
                logger.info(f"ğŸ¯ API ì‘ë‹µ: {code} - {msg}")
                
                if code == '00':
                    items = root.findall('.//item')
                    logger.info(f"âœ… API í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì•„ì´í…œ ìˆ˜: {len(items)}")
                    return True
                elif code == '99':
                    logger.info(f"âš ï¸ ë°ì´í„° ì—†ìŒ (ì •ìƒ) - {lawd_cd}")
                    return True
                elif code == '04':
                    logger.error(f"âŒ SERVICE_KEY ì˜¤ë¥˜")
                    return False
                elif code == '05':
                    logger.error(f"âŒ ì„œë¹„ìŠ¤ ì ‘ê·¼ ê¶Œí•œ ì—†ìŒ")
                    return False
                else:
                    logger.error(f"âŒ API ì˜¤ë¥˜: {code} - {msg}")
                    return False
            else:
                logger.error("âŒ ê²°ê³¼ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
        else:
            logger.error(f"âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ API í…ŒìŠ¤íŠ¸ ì˜ˆì™¸: {e}")
        return False

def fetch_data_simple(lawd_cd, deal_ymd, service_key):
    """ë°ì´í„° ìˆ˜ì§‘"""
    params = {
        'serviceKey': service_key,
        'LAWD_CD': lawd_cd,
        'DEAL_YMD': deal_ymd,
        'pageNo': '1',
        'numOfRows': '1000'
    }
    
    try:
        response = requests.get(BASE_URL, params=params, timeout=30)
        
        if response.status_code == 200:
            root = ET.fromstring(response.content)
            result_code = root.find('.//resultCode')
            
            if result_code is not None and result_code.text == '00':
                items_element = root.find('.//items')
                if items_element:
                    items_data = []
                    for item in items_element.findall('item'):
                        item_dict = {}
                        for child in item:
                            item_dict[child.tag] = child.text.strip() if child.text else ''
                        items_data.append(item_dict)
                    return items_data
                else:
                    return []
            elif result_code is not None and result_code.text == '99':
                return []  # ë°ì´í„° ì—†ìŒ
            else:
                logger.warning(f"âš ï¸ {lawd_cd} API ì‘ë‹µ: {result_code.text if result_code is not None else 'Unknown'}")
                return []
        else:
            logger.warning(f"âš ï¸ {lawd_cd} HTTP ì˜¤ë¥˜: {response.status_code}")
            return []
            
    except Exception as e:
        logger.error(f"âŒ {lawd_cd} ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)[:50]}")
        return []

def create_unique_id(df):
    """ê³ ìœ  ID ìƒì„±"""
    if df.empty:
        return df
    
    id_cols = ['ê±°ë˜ê¸ˆì•¡', 'ë…„', 'ì›”', 'ì¼', 'ì „ìš©ë©´ì ', 'ì§€ë²ˆ', 'ì¸µ']
    valid_cols = [col for col in id_cols if col in df.columns]
    
    if valid_cols:
        df['unique_id'] = df[valid_cols].astype(str).agg('_'.join, axis=1)
    return df

def upload_to_sheet(df_new, df_existing, worksheet):
    """Google Sheetsì— ì—…ë¡œë“œ"""
    if df_new.empty:
        logger.info("ğŸ“­ ì—…ë¡œë“œí•  ìƒˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0, df_existing
    
    logger.info(f"ğŸ“Š ìƒˆ ë°ì´í„°: {len(df_new)}ê±´")
    logger.info(f"ğŸ“‹ ìƒˆ ë°ì´í„° ì»¬ëŸ¼: {list(df_new.columns)}")
    
    # ê³ ìœ  ID ìƒì„±
    df_new = create_unique_id(df_new)
    
    if not df_existing.empty:
        if 'unique_id' not in df_existing.columns:
            df_existing = create_unique_id(df_existing)
        
        # ì¤‘ë³µ ì œê±°
        newly_added = df_new[~df_new['unique_id'].isin(df_existing['unique_id'])].copy()
        logger.info(f"ğŸ” ì¤‘ë³µ ì œê±° í›„: {len(newly_added)}ê±´")
    else:
        newly_added = df_new.copy()
        logger.info("ğŸ“ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ - ëª¨ë“  ë°ì´í„° ì¶”ê°€")
    
    if newly_added.empty:
        logger.info("â„¹ï¸ ì¶”ê°€í•  ìƒˆ ë°ì´í„° ì—†ìŒ (ëª¨ë‘ ì¤‘ë³µ)")
        return 0, df_existing
    
    count = len(newly_added)
    df_to_upload = newly_added.drop(columns=['unique_id'], errors='ignore')
    
    try:
        if worksheet.row_count < 2:
            logger.info("ğŸ“ ë¹ˆ ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€")
            set_with_dataframe(worksheet, df_to_upload, include_index=False)
        else:
            logger.info("ğŸ“ ê¸°ì¡´ ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€")
            headers = worksheet.row_values(1)
            
            # í—¤ë”ì— ë§ì¶° ë°ì´í„° ì •ë ¬
            aligned = pd.DataFrame(columns=headers)
            for col in headers:
                if col in df_to_upload.columns:
                    aligned[col] = df_to_upload[col]
                else:
                    aligned[col] = ''
            
            worksheet.append_rows(aligned.values.tolist(), value_input_option='USER_ENTERED')
        
        df_existing_updated = pd.concat([df_existing, newly_added], ignore_index=True)
        logger.info(f"âœ… ì—…ë¡œë“œ ì„±ê³µ: {count}ê±´ ì¶”ê°€ë¨")
        return count, df_existing_updated
        
    except Exception as e:
        logger.error(f"âŒ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        return -1, df_existing

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    start_time = time.time()
    
    print("ğŸš€ ===== ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì—…ë°ì´íŠ¸ ì‹œì‘ =====")
    logger.info("ğŸš€ ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì—…ë°ì´íŠ¸ ì‹œì‘")
    logger.info(f"ğŸ”§ ì‹¤í–‰ ëª¨ë“œ: {RUN_MODE}")
    logger.info(f"ğŸ“… ëŒ€ìƒ ì›”: {MONTHS_TO_FETCH}")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not SERVICE_KEY:
        logger.error("âŒ SERVICE_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("âŒ SERVICE_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    if not GOOGLE_CREDENTIALS_JSON:
        logger.error("âŒ GOOGLE_CREDENTIALS_JSONì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("âŒ GOOGLE_CREDENTIALS_JSONì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"ğŸ”‘ SERVICE_KEY ê¸¸ì´: {len(SERVICE_KEY)}")
    print(f"âœ… í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ì™„ë£Œ - SERVICE_KEY: {len(SERVICE_KEY)}ì")
    
    # ì§€ì—­ ì½”ë“œ ë¡œë“œ
    lawd_codes = get_lawd_codes(LAWD_CODE_FILE)
    if not lawd_codes:
        logger.error("âŒ ìœ íš¨í•œ ì§€ì—­ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("âŒ ìœ íš¨í•œ ì§€ì—­ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì§€ì—­ ì½”ë“œ ë¡œë“œ ì™„ë£Œ: {len(lawd_codes)}ê°œ")
    
    # API ì—°ê²° í…ŒìŠ¤íŠ¸
    print("ğŸ§ª API ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    if not test_api_connection(lawd_codes[0], MONTHS_TO_FETCH[0], SERVICE_KEY):
        logger.error("âŒ API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print("âŒ API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - SERVICE_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    print("âœ… API ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    
    # Google Sheets ì´ˆê¸°í™”
    print("ğŸ“Š Google Sheets ì´ˆê¸°í™” ì¤‘...")
    try:
        creds = get_google_creds()
        if not creds:
            return
        
        gc = gspread.service_account_from_dict(creds)
        
        try:
            sh = gc.open(GOOGLE_SHEET_NAME)
            logger.info(f"âœ… ê¸°ì¡´ ì‹œíŠ¸ ì—´ê¸°: {GOOGLE_SHEET_NAME}")
        except gspread.exceptions.SpreadsheetNotFound:
            sh = gc.create(GOOGLE_SHEET_NAME)
            logger.info(f"ğŸ†• ìƒˆ ì‹œíŠ¸ ìƒì„±: {GOOGLE_SHEET_NAME}")
        
        worksheet = sh.get_worksheet(0)
        
        # ì‹œíŠ¸ URL ì¶œë ¥
        sheet_url = sh.url
        logger.info(f"ğŸ”— ì‹œíŠ¸ URL: {sheet_url}")
        print(f"ğŸ”— êµ¬ê¸€ ì‹œíŠ¸: {sheet_url}")
        
        # ê¸°ì¡´ ë°ì´í„° ì½ê¸°
        existing_records = worksheet.get_all_records()
        df_existing = pd.DataFrame(existing_records)
        
        if not df_existing.empty:
            df_existing.columns = df_existing.columns.str.strip()
            logger.info(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(df_existing)}ê±´")
            print(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(df_existing)}ê±´")
        else:
            logger.info("ğŸ“ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ")
            print("ğŸ“ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ - ìƒˆë¡œ ì‹œì‘")
            
    except Exception as e:
        logger.error(f"âŒ Google Sheets ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        print(f"âŒ Google Sheets ì˜¤ë¥˜: {e}")
        return
    
    total_added = 0
    
    # ë°ì´í„° ìˆ˜ì§‘ ë° ì—…ë¡œë“œ
    for month in MONTHS_TO_FETCH:
        print(f"\nğŸ“… {month} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        logger.info(f"ğŸ“… {month} ì²˜ë¦¬ ì‹œì‘")
        
        monthly_data = []
        
        for i, code in enumerate(lawd_codes):
            print(f"ğŸ”„ [{i+1}/{len(lawd_codes)}] {code} ì²˜ë¦¬ ì¤‘...")
            data = fetch_data_simple(code, month, SERVICE_KEY)
            
            if data:
                monthly_data.extend(data)
                logger.info(f"âœ… {code}: {len(data)}ê±´ ìˆ˜ì§‘")
                print(f"   âœ… {code}: {len(data)}ê±´ ìˆ˜ì§‘")
            else:
                logger.info(f"ğŸ“­ {code}: ë°ì´í„° ì—†ìŒ")
                print(f"   ğŸ“­ {code}: ë°ì´í„° ì—†ìŒ")
            
            time.sleep(0.5)  # API í˜¸ì¶œ ê°„ê²©
        
        print(f"ğŸ“Š {month} ì´ ìˆ˜ì§‘: {len(monthly_data)}ê±´")
        logger.info(f"ğŸ“Š {month} ì´ ìˆ˜ì§‘: {len(monthly_data)}ê±´")
        
        if not monthly_data:
            print(f"âš ï¸ {month}: ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
            continue
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì—…ë¡œë“œ
        df_month = pd.DataFrame(monthly_data)
        df_month.columns = df_month.columns.str.strip()
        
        print(f"ğŸ“¤ Google Sheetsì— ì—…ë¡œë“œ ì¤‘...")
        added, df_existing = upload_to_sheet(df_month, df_existing, worksheet)
        
        if added > 0:
            total_added += added
            print(f"âœ… {month}: {added}ê±´ ì¶”ê°€ë¨")
            logger.info(f"âœ… {month}: {added}ê±´ ì¶”ê°€")
        elif added == 0:
            print(f"â„¹ï¸ {month}: ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ (ì¤‘ë³µ)")
            logger.info(f"â„¹ï¸ {month}: ì¤‘ë³µìœ¼ë¡œ ì¶”ê°€ ì—†ìŒ")
        else:
            print(f"âŒ {month}: ì—…ë¡œë“œ ì‹¤íŒ¨")
            logger.error(f"âŒ {month}: ì—…ë¡œë“œ ì‹¤íŒ¨")
    
    # ì™„ë£Œ
    elapsed = time.time() - start_time
    print(f"\nğŸ‰ ===== ì™„ë£Œ =====")
    print(f"ğŸ“Š ì´ {total_added}ê±´ ì¶”ê°€")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed//60:.0f}ë¶„ {elapsed%60:.0f}ì´ˆ")
    print(f"ğŸ”— ê²°ê³¼ í™•ì¸: {sheet_url}")
    
    logger.info(f"ğŸ‰ ì™„ë£Œ - ì´ {total_added}ê±´ ì¶”ê°€, {elapsed//60:.0f}ë¶„ {elapsed%60:.0f}ì´ˆ")
    
    if total_added == 0:
        print("\nâš ï¸ ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("ê°€ëŠ¥í•œ ì›ì¸:")
        print("1. í•´ë‹¹ ì›”ì— ì‹¤ê±°ë˜ ë°ì´í„°ê°€ ì—†ìŒ")
        print("2. ì´ë¯¸ ëª¨ë“  ë°ì´í„°ê°€ ì‹œíŠ¸ì— ì¡´ì¬")
        logger.warning("ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° ì—†ìŒ")
    else:
        print(f"\nğŸ‰ ì„±ê³µ! Google Sheetsì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”!")

if __name__ == '__main__':
    main()
