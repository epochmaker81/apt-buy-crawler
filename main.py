import os
import requests
import pandas as pd
import gspread
from gspread_dataframe import set_with_dataframe
import xml.etree.ElementTree as ET
import time
import json
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
import logging
import traceback
import random

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ì„¤ì • ë³€ìˆ˜ ---
SERVICE_KEY = os.getenv('SERVICE_KEY')
GOOGLE_CREDENTIALS_JSON = os.getenv('GOOGLE_CREDENTIALS_JSON')
GOOGLE_SHEET_NAME = 'ì „êµ­ ì•„íŒŒíŠ¸ ë§¤ë§¤ ì‹¤ê±°ë˜ê°€_ëˆ„ì '
LAWD_CODE_FILE = 'lawd_code.csv'

# ëŒ€ì•ˆ API URLë“¤
API_URLS = [
    'http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade',
    'https://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade',
    'http://apis.data.go.kr/1613000/RTMSDataSvcAptTrade/getRTMSDataSvcAptTrade'  # ëŒ€ì•ˆ URL
]

# ì‹¤í–‰ ëª¨ë“œ ì„¤ì •
RUN_MODE = os.getenv('RUN_MODE', 'TEST')

# ë‚ ì§œ ì„¤ì •
today_kst = datetime.utcnow() + timedelta(hours=9)

if RUN_MODE == 'TEST':
    MONTHS_TO_FETCH = [today_kst.strftime('%Y%m')]
    TARGET_REGIONS = ['11110']  # ì¢…ë¡œêµ¬ 1ê°œë§Œ
elif RUN_MODE == 'QUICK':
    MONTHS_TO_FETCH = [today_kst.strftime('%Y%m')]
    TARGET_REGIONS = None
else:
    MONTHS_TO_FETCH = []
    for i in range(3):
        target_date = today_kst - relativedelta(months=i)
        MONTHS_TO_FETCH.append(target_date.strftime('%Y%m'))
    TARGET_REGIONS = None

def test_basic_network():
    """ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("ğŸŒ ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸...")
    
    test_sites = [
        'http://www.google.com',
        'http://httpbin.org/get',
        'https://api.github.com',
        'http://data.go.kr'
    ]
    
    for site in test_sites:
        try:
            response = requests.get(site, timeout=10)
            print(f"   âœ… {site}: {response.status_code}")
        except Exception as e:
            print(f"   âŒ {site}: {str(e)[:50]}")

def generate_sample_data(lawd_cd, deal_ymd):
    """ìƒ˜í”Œ ë°ì´í„° ìƒì„± (API ì ‘ê·¼ ë¶ˆê°€ ì‹œ ëŒ€ì•ˆ)"""
    print(f"ğŸ­ [{lawd_cd}] ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ì‹¤ì œì™€ ìœ ì‚¬í•œ ìƒ˜í”Œ ë°ì´í„°
    sample_data = []
    
    for i in range(random.randint(0, 5)):  # 0-5ê±´ ëœë¤ ìƒì„±
        data = {
            'ê±°ë˜ê¸ˆì•¡': f"{random.randint(30000, 150000)}",  # 3ì–µ~15ì–µ
            'ê±´ì¶•ë…„ë„': str(random.randint(1990, 2020)),
            'ë…„': deal_ymd[:4],
            'ì›”': deal_ymd[4:6],
            'ì¼': f"{random.randint(1, 28):02d}",
            'ì „ìš©ë©´ì ': f"{random.randint(60, 150)}.{random.randint(10, 99)}",
            'ì§€ë²ˆ': f"{random.randint(1, 999)}",
            'ì¸µ': str(random.randint(1, 20)),
            'ë²•ì •ë™': f"í…ŒìŠ¤íŠ¸ë™{random.randint(1, 5)}ê°€",
            'ì•„íŒŒíŠ¸': f"í…ŒìŠ¤íŠ¸ì•„íŒŒíŠ¸{random.randint(1, 10)}",
            'ë²•ì •ë™ì‹œêµ°êµ¬ì½”ë“œ': lawd_cd,
            'ë²•ì •ë™ìë©´ë™ì½”ë“œ': f"{random.randint(10100, 10999)}",
            'ë„ë¡œëª…': f"í…ŒìŠ¤íŠ¸ë¡œ{random.randint(1, 100)}",
            'í•´ì œì‚¬ìœ ë°œìƒì¼': '',
            'ê±°ë˜ìœ í˜•': 'ì§ê±°ë˜',
            'ì¤‘ê°œì‚¬ì†Œì¬ì§€': '',
            'í•´ì œì—¬ë¶€': 'O'
        }
        sample_data.append(data)
    
    if sample_data:
        print(f"   ğŸ­ [{lawd_cd}] ìƒ˜í”Œ {len(sample_data)}ê±´ ìƒì„±ë¨")
    else:
        print(f"   ğŸ“­ [{lawd_cd}] ìƒ˜í”Œ ë°ì´í„° ì—†ìŒ")
    
    return sample_data

def try_api_with_fallback(lawd_cd, deal_ymd, service_key):
    """API ì‹œë„ í›„ ì‹¤íŒ¨ ì‹œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    print(f"ğŸ”„ [{lawd_cd}] ì‹¤ì œ API ì‹œë„ ì¤‘...")
    
    params = {
        'serviceKey': service_key,
        'LAWD_CD': lawd_cd,
        'DEAL_YMD': deal_ymd,
        'pageNo': '1',
        'numOfRows': '100'
    }
    
    # ëª¨ë“  API URL ì‹œë„
    for url_idx, base_url in enumerate(API_URLS):
        print(f"   ğŸŒ [{lawd_cd}] API URL {url_idx+1}/{len(API_URLS)} ì‹œë„...")
        
        for attempt in range(2):  # ê° URLë‹¹ 2ë²ˆì”©ë§Œ ì‹œë„
            try:
                print(f"      ğŸ“¡ [{lawd_cd}] ì‹œë„ {attempt+1}/2...")
                
                # requests ì„¤ì •
                session = requests.Session()
                session.headers.update({
                    'User-Agent': 'Mozilla/5.0 (compatible; DataCollector/1.0)',
                    'Accept': 'application/xml, text/xml, */*'
                })
                
                response = session.get(
                    base_url,
                    params=params,
                    timeout=15,  # íƒ€ì„ì•„ì›ƒ ë‹¨ì¶•
                    verify=False
                )
                
                print(f"      ğŸ“¡ [{lawd_cd}] HTTP: {response.status_code}")
                
                if response.status_code == 200:
                    try:
                        root = ET.fromstring(response.content)
                        result_code = root.find('.//resultCode')
                        
                        if result_code is not None:
                            code = result_code.text
                            print(f"      ğŸ¯ [{lawd_cd}] ê²°ê³¼: {code}")
                            
                            if code == '00':
                                items_element = root.find('.//items')
                                if items_element:
                                    items = items_element.findall('item')
                                    if items:
                                        print(f"      âœ… [{lawd_cd}] ì‹¤ì œ API ì„±ê³µ! {len(items)}ê±´")
                                        
                                        items_data = []
                                        for item in items:
                                            item_dict = {}
                                            for child in item:
                                                item_dict[child.tag] = child.text.strip() if child.text else ''
                                            items_data.append(item_dict)
                                        
                                        session.close()
                                        return items_data, True  # True = ì‹¤ì œ ë°ì´í„°
                                    else:
                                        print(f"      ğŸ“­ [{lawd_cd}] ë°ì´í„° ì—†ìŒ")
                                        session.close()
                                        return [], True
                            elif code == '99':
                                print(f"      ğŸ“­ [{lawd_cd}] ë°ì´í„° ì—†ìŒ (ì •ìƒ)")
                                session.close()
                                return [], True
                            else:
                                print(f"      âŒ [{lawd_cd}] API ì˜¤ë¥˜: {code}")
                        
                    except ET.ParseError:
                        print(f"      âŒ [{lawd_cd}] XML íŒŒì‹± ì˜¤ë¥˜")
                
                session.close()
                
            except Exception as e:
                print(f"      âŒ [{lawd_cd}] ì—°ê²° ì˜¤ë¥˜: {str(e)[:30]}")
            
            if attempt < 1:
                time.sleep(1)
    
    # ëª¨ë“  API ì‹œë„ ì‹¤íŒ¨ â†’ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    print(f"   ğŸ­ [{lawd_cd}] API ì‹¤íŒ¨ â†’ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
    sample_data = generate_sample_data(lawd_cd, deal_ymd)
    return sample_data, False  # False = ìƒ˜í”Œ ë°ì´í„°

def get_google_creds():
    """Google ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    if GOOGLE_CREDENTIALS_JSON is None:
        logger.error("âŒ GOOGLE_CREDENTIALS_JSONì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    try:
        creds = json.loads(GOOGLE_CREDENTIALS_JSON)
        logger.info("âœ… Google ì¸ì¦ ì •ë³´ ë¡œë“œ ì„±ê³µ")
        return creds
    except json.JSONDecodeError as e:
        logger.error(f"âŒ GOOGLE_CREDENTIALS_JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None

def get_lawd_codes(filepath):
    """ì§€ì—­ ì½”ë“œ íŒŒì¼ ì½ê¸°"""
    try:
        if not os.path.exists(filepath):
            logger.error(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filepath}")
            return []
        
        df = pd.read_csv(filepath)
        logger.info(f"ğŸ“ CSV íŒŒì¼ ì½ê¸° ì„±ê³µ: {len(df)}ê°œ ì§€ì—­")
        
        if 'code' not in df.columns:
            logger.error("âŒ 'code' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        codes = df['code'].astype(str).str.strip().tolist()
        
        # ìœ íš¨í•œ ì½”ë“œë§Œ í•„í„°ë§
        valid_codes = []
        for code in codes:
            if code and code != 'nan' and len(code) == 5 and code.isdigit():
                if not code.endswith('000'):
                    valid_codes.append(code)
        
        logger.info(f"âœ… ìœ íš¨í•œ ì§€ì—­ ì½”ë“œ: {len(valid_codes)}ê°œ")
        
        # í•„í„°ë§ ì ìš©
        if TARGET_REGIONS:
            codes = [c for c in valid_codes if c in TARGET_REGIONS]
            logger.info(f"ğŸ¯ TARGET_REGIONS í•„í„°ë§ í›„: {len(codes)}ê°œ - {codes}")
        elif RUN_MODE == 'QUICK':
            major_city_prefixes = ['11', '26', '27', '28', '29', '30', '31', '36']
            codes = [c for c in valid_codes if any(c.startswith(p) for p in major_city_prefixes)]
            logger.info(f"ğŸ™ï¸ ì£¼ìš” ë„ì‹œ í•„í„°ë§ í›„: {len(codes)}ê°œ")
        else:
            codes = valid_codes
        
        if not codes:
            logger.error("âŒ ì²˜ë¦¬í•  ìœ íš¨í•œ ì§€ì—­ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        return codes
        
    except Exception as e:
        logger.error(f"âŒ ì§€ì—­ ì½”ë“œ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return []

def create_unique_id(df):
    """ê³ ìœ  ID ìƒì„±"""
    if df.empty:
        return df
    
    id_cols = ['ê±°ë˜ê¸ˆì•¡', 'ë…„', 'ì›”', 'ì¼', 'ì „ìš©ë©´ì ', 'ì§€ë²ˆ', 'ì¸µ']
    valid_cols = [col for col in id_cols if col in df.columns]
    
    if valid_cols:
        df['unique_id'] = df[valid_cols].astype(str).agg('_'.join, axis=1)
    return df

def upload_to_sheet(df_new, df_existing, worksheet, is_real_data=True):
    """Google Sheetsì— ì—…ë¡œë“œ"""
    if df_new.empty:
        print("ğŸ“­ ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0, df_existing
    
    data_type = "ì‹¤ì œ ë°ì´í„°" if is_real_data else "ìƒ˜í”Œ ë°ì´í„°"
    print(f"ğŸ“Š ìƒˆ {data_type}: {len(df_new)}ê±´")
    print(f"ğŸ“‹ ì»¬ëŸ¼: {list(df_new.columns)}")
    
    df_new = create_unique_id(df_new)
    
    if not df_existing.empty:
        if 'unique_id' not in df_existing.columns:
            df_existing = create_unique_id(df_existing)
        
        newly_added = df_new[~df_new['unique_id'].isin(df_existing['unique_id'])].copy()
        print(f"ğŸ” ì¤‘ë³µ ì œê±° í›„: {len(newly_added)}ê±´")
    else:
        newly_added = df_new.copy()
        print("ğŸ“ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ - ëª¨ë“  ë°ì´í„° ì¶”ê°€")
    
    if newly_added.empty:
        print("â„¹ï¸ ì¶”ê°€í•  ìƒˆ ë°ì´í„° ì—†ìŒ (ëª¨ë‘ ì¤‘ë³µ)")
        return 0, df_existing
    
    # ë°ì´í„° íƒ€ì… í‘œì‹œ ì»¬ëŸ¼ ì¶”ê°€
    newly_added['ë°ì´í„°_íƒ€ì…'] = data_type
    newly_added['ìˆ˜ì§‘_ì‹œê°„'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    count = len(newly_added)
    df_to_upload = newly_added.drop(columns=['unique_id'], errors='ignore')
    
    try:
        if worksheet.row_count < 2:
            print("ğŸ“ ë¹ˆ ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€")
            set_with_dataframe(worksheet, df_to_upload, include_index=False)
        else:
            print("ğŸ“ ê¸°ì¡´ ì‹œíŠ¸ì— ë°ì´í„° ì¶”ê°€")
            
            # ìƒˆ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í—¤ë” ì—…ë°ì´íŠ¸
            existing_headers = worksheet.row_values(1)
            new_headers = list(df_to_upload.columns)
            
            if set(new_headers) != set(existing_headers):
                print("ğŸ“‹ í—¤ë” ì—…ë°ì´íŠ¸ í•„ìš”")
                # ê¸°ì¡´ ë°ì´í„° ì½ê¸°
                all_data = worksheet.get_all_records()
                existing_df = pd.DataFrame(all_data)
                
                # ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
                for col in new_headers:
                    if col not in existing_df.columns:
                        existing_df[col] = ''
                
                # ì „ì²´ ì¬ì—…ë¡œë“œ
                combined_df = pd.concat([existing_df, df_to_upload], ignore_index=True)
                worksheet.clear()
                set_with_dataframe(worksheet, combined_df, include_index=False)
            else:
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¶”ê°€
                worksheet.append_rows(df_to_upload.values.tolist(), value_input_option='USER_ENTERED')
        
        df_existing_updated = pd.concat([df_existing, newly_added], ignore_index=True)
        print(f"âœ… ì—…ë¡œë“œ ì„±ê³µ: {count}ê±´ {data_type} ì¶”ê°€ë¨")
        return count, df_existing_updated
        
    except Exception as e:
        print(f"âŒ ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        logger.error(f"ì—…ë¡œë“œ ì˜¤ë¥˜: {e}")
        return -1, df_existing

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    start_time = time.time()
    
    print("ğŸš€ ===== ì•„íŒŒíŠ¸ ì‹¤ê±°ë˜ê°€ ì—…ë°ì´íŠ¸ ì‹œì‘ =====")
    print(f"ğŸ”§ ì‹¤í–‰ ëª¨ë“œ: {RUN_MODE}")
    print(f"ğŸ“… ëŒ€ìƒ ì›”: {MONTHS_TO_FETCH}")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    if not SERVICE_KEY:
        print("âŒ SERVICE_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    if not GOOGLE_CREDENTIALS_JSON:
        print("âŒ GOOGLE_CREDENTIALS_JSONì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… SERVICE_KEY: {len(SERVICE_KEY)}ì")
    print(f"âœ… GOOGLE_CREDENTIALS_JSON: ì„¤ì •ë¨")
    
    # ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸
    test_basic_network()
    
    # ì§€ì—­ ì½”ë“œ ë¡œë“œ
    lawd_codes = get_lawd_codes(LAWD_CODE_FILE)
    if not lawd_codes:
        print("âŒ ìœ íš¨í•œ ì§€ì—­ ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"âœ… ì§€ì—­ ì½”ë“œ: {len(lawd_codes)}ê°œ - {lawd_codes}")
    
    # Google Sheets ì´ˆê¸°í™”
    print("ğŸ“Š Google Sheets ì´ˆê¸°í™” ì¤‘...")
    try:
        creds = get_google_creds()
        if not creds:
            return
        
        gc = gspread.service_account_from_dict(creds)
        print("âœ… Google Sheets í´ë¼ì´ì–¸íŠ¸ ìƒì„±")
        
        try:
            sh = gc.open(GOOGLE_SHEET_NAME)
            print(f"âœ… ê¸°ì¡´ ì‹œíŠ¸ ì—´ê¸°: {GOOGLE_SHEET_NAME}")
        except gspread.exceptions.SpreadsheetNotFound:
            sh = gc.create(GOOGLE_SHEET_NAME)
            print(f"ğŸ†• ìƒˆ ì‹œíŠ¸ ìƒì„±: {GOOGLE_SHEET_NAME}")
        
        worksheet = sh.get_worksheet(0)
        sheet_url = sh.url
        print(f"ğŸ”— ì‹œíŠ¸ URL: {sheet_url}")
        
        # ê¸°ì¡´ ë°ì´í„° ì½ê¸°
        existing_records = worksheet.get_all_records()
        df_existing = pd.DataFrame(existing_records)
        
        if not df_existing.empty:
            df_existing.columns = df_existing.columns.str.strip()
            print(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(df_existing)}ê±´")
        else:
            print("ğŸ“ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ")
            
    except Exception as e:
        print(f"âŒ Google Sheets ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        return
    
    total_added = 0
    real_data_count = 0
    sample_data_count = 0
    
    # ë°ì´í„° ìˆ˜ì§‘ ë° ì—…ë¡œë“œ
    for month in MONTHS_TO_FETCH:
        print(f"\nğŸ“… ===== {month} ë°ì´í„° ìˆ˜ì§‘ =====")
        
        monthly_data = []
        monthly_real_data = True
        
        for i, code in enumerate(lawd_codes):
            print(f"\n[{i+1}/{len(lawd_codes)}] {code} ì²˜ë¦¬...")
            data, is_real = try_api_with_fallback(code, month, SERVICE_KEY)
            
            if data:
                monthly_data.extend(data)
                if is_real:
                    real_data_count += len(data)
                    print(f"   âœ… ì‹¤ì œ ë°ì´í„° {len(data)}ê±´ ìˆ˜ì§‘")
                else:
                    sample_data_count += len(data)
                    print(f"   ğŸ­ ìƒ˜í”Œ ë°ì´í„° {len(data)}ê±´ ìƒì„±")
                    monthly_real_data = False
            else:
                print(f"   ğŸ“­ ë°ì´í„° ì—†ìŒ")
            
            time.sleep(1)
        
        print(f"\nğŸ“Š {month} ì´ ìˆ˜ì§‘: {len(monthly_data)}ê±´")
        
        if not monthly_data:
            print(f"âš ï¸ {month}: ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
            continue
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df_month = pd.DataFrame(monthly_data)
        df_month.columns = df_month.columns.str.strip()
        
        print(f"ğŸ“¤ Google Sheetsì— ì—…ë¡œë“œ ì¤‘...")
        added, df_existing = upload_to_sheet(df_month, df_existing, worksheet, monthly_real_data)
        
        if added > 0:
            total_added += added
            print(f"âœ… {month}: {added}ê±´ ì¶”ê°€ë¨")
        elif added == 0:
            print(f"â„¹ï¸ {month}: ìƒˆë¡œìš´ ë°ì´í„° ì—†ìŒ")
        else:
            print(f"âŒ {month}: ì—…ë¡œë“œ ì‹¤íŒ¨")
    
    # ì™„ë£Œ
    elapsed = time.time() - start_time
    print(f"\nğŸ‰ ===== ì™„ë£Œ =====")
    print(f"ğŸ“Š ì´ {total_added}ê±´ ì¶”ê°€")
    print(f"ğŸ”— ì‹¤ì œ ë°ì´í„°: {real_data_count}ê±´")
    print(f"ğŸ­ ìƒ˜í”Œ ë°ì´í„°: {sample_data_count}ê±´")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed//60:.0f}ë¶„ {elapsed%60:.0f}ì´ˆ")
    print(f"ğŸ”— ê²°ê³¼: {sheet_url}")
    
    if real_data_count > 0:
        print(f"\nğŸ‰ ì„±ê³µ! ì‹¤ì œ APIì—ì„œ {real_data_count}ê±´ ìˆ˜ì§‘ë¨")
    elif sample_data_count > 0:
        print(f"\nâš ï¸ API ì ‘ê·¼ ë¶ˆê°€ë¡œ ìƒ˜í”Œ ë°ì´í„° {sample_data_count}ê±´ ìƒì„±ë¨")
        print("ğŸ“ ì°¸ê³ : ìƒ˜í”Œ ë°ì´í„°ëŠ” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ìš©ì…ë‹ˆë‹¤.")
    else:
        print(f"\nğŸ“­ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    main()
